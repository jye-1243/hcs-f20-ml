{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all data\n",
    "# Remove first col b/c data was poorly cleaned by me\n",
    "# Data from: https://www.kaggle.com/zalando-research/fashionmnist\n",
    "zero_data = np.delete(np.load('./zeros.npy'), 0, 1)\n",
    "one_data = np.delete(np.load('./ones.npy'), 0, 1)\n",
    "two_data = np.delete(np.load('./twos.npy'), 0, 1)\n",
    "three_data = np.delete(np.load('./threes.npy'), 0, 1)\n",
    "four_data = np.delete(np.load('./fours.npy'), 0, 1)\n",
    "five_data = np.delete(np.load('./fives.npy'), 0, 1)\n",
    "six_data = np.delete(np.load('./sixes.npy'), 0, 1)\n",
    "seven_data = np.delete(np.load('./sevens.npy'), 0, 1)\n",
    "eight_data = np.delete(np.load('./eights.npy'), 0, 1)\n",
    "nine_data = np.delete(np.load('./nines.npy'), 0, 1)\n",
    "t_data = np.delete(np.load('./tens.npy'), 0, 1)\n",
    "h_data = np.delete(np.load('./hundreds.npy'), 0, 1)\n",
    "th_data = np.delete(np.load('./thousands.npy'), 0, 1)\n",
    "tt_data = np.delete(np.load('./tenthous.npy'), 0, 1)\n",
    "tm_data = np.delete(np.load('./tenmils.npy'), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "4096\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# understanding the data\n",
    "def print_data(data):\n",
    "    for line in data:\n",
    "        print(line)\n",
    "        print('----------------')\n",
    "\n",
    "print(len(zero_data))\n",
    "print(len(zero_data[0]))\n",
    "print(zero_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomizing the data\n",
    "np.random.shuffle(zero_data)\n",
    "np.random.shuffle(one_data)\n",
    "np.random.shuffle(two_data)\n",
    "np.random.shuffle(three_data)\n",
    "np.random.shuffle(four_data)\n",
    "np.random.shuffle(five_data)\n",
    "np.random.shuffle(six_data)\n",
    "np.random.shuffle(seven_data)\n",
    "np.random.shuffle(eight_data)\n",
    "np.random.shuffle(nine_data)\n",
    "np.random.shuffle(t_data)\n",
    "np.random.shuffle(h_data)\n",
    "np.random.shuffle(th_data)\n",
    "np.random.shuffle(tm_data)\n",
    "\n",
    "# splitting into test and train\n",
    "train_percent = 0.8\n",
    "\n",
    "# make datasets for training and testing\n",
    "zero_cutoff = math.floor(len(zero_data) * train_percent)\n",
    "zero_trainX = zero_data[:zero_cutoff]\n",
    "zero_trainY = np.tile([1,0,0,0,0,0,0,0,0,0,0,0,0,0,0], (len(zero_trainX),1))\n",
    "zero_testX = zero_data[zero_cutoff:]\n",
    "zero_testY = np.tile([1,0,0,0,0,0,0,0,0,0,0,0,0,0,0], (len(zero_testX),1))\n",
    "\n",
    "one_cutoff = math.floor(len(one_data) * train_percent)\n",
    "one_trainX = one_data[:one_cutoff]\n",
    "one_trainY = np.tile([0,1,0,0,0,0,0,0,0,0,0,0,0,0,0], (len(one_trainX),1))\n",
    "one_testX = one_data[one_cutoff:]\n",
    "one_testY = np.tile([0,1,0,0,0,0,0,0,0,0,0,0,0,0,0], (len(one_testX),1))\n",
    "\n",
    "two_cutoff = math.floor(len(two_data) * train_percent)\n",
    "two_trainX = two_data[:two_cutoff]\n",
    "two_trainY = np.tile([0,0,1,0,0,0,0,0,0,0,0,0,0,0,0], (len(two_trainX),1))\n",
    "two_testX = two_data[two_cutoff:]\n",
    "two_testY = np.tile([0,0,1,0,0,0,0,0,0,0,0,0,0,0,0], (len(two_testX),1))\n",
    "\n",
    "three_cutoff = math.floor(len(three_data) * train_percent)\n",
    "three_trainX = three_data[:three_cutoff]\n",
    "three_trainY = np.tile([0,0,0,1,0,0,0,0,0,0,0,0,0,0,0], (len(three_trainX),1))\n",
    "three_testX = three_data[three_cutoff:]\n",
    "three_testY = np.tile([0,0,0,1,0,0,0,0,0,0,0,0,0,0,0], (len(three_testX),1))\n",
    "\n",
    "four_cutoff = math.floor(len(four_data) * train_percent)\n",
    "four_trainX = four_data[:four_cutoff]\n",
    "four_trainY = np.tile([0,0,0,0,1,0,0,0,0,0,0,0,0,0,0], (len(four_trainX),1))\n",
    "four_testX = four_data[four_cutoff:]\n",
    "four_testY = np.tile([0,0,0,0,1,0,0,0,0,0,0,0,0,0,0], (len(four_testX),1))\n",
    "\n",
    "five_cutoff = math.floor(len(five_data) * train_percent)\n",
    "five_trainX = five_data[:five_cutoff]\n",
    "five_trainY = np.tile([0,0,0,0,0,1,0,0,0,0,0,0,0,0,0], (len(five_trainX),1))\n",
    "five_testX = five_data[five_cutoff:]\n",
    "five_testY = np.tile([0,0,0,0,0,1,0,0,0,0,0,0,0,0,0], (len(five_testX),1))\n",
    "\n",
    "six_cutoff = math.floor(len(six_data) * train_percent)\n",
    "six_trainX = six_data[:six_cutoff]\n",
    "six_trainY = np.tile([0,0,0,0,0,0,1,0,0,0,0,0,0,0,0], (len(six_trainX),1))\n",
    "six_testX = six_data[six_cutoff:]\n",
    "six_testY = np.tile([0,0,0,0,0,0,1,0,0,0,0,0,0,0,0], (len(six_testX),1))\n",
    "\n",
    "seven_cutoff = math.floor(len(seven_data) * train_percent)\n",
    "seven_trainX = seven_data[:seven_cutoff]\n",
    "seven_trainY = np.tile([0,0,0,0,0,0,0,1,0,0,0,0,0,0,0], (len(seven_trainX),1))\n",
    "seven_testX = seven_data[seven_cutoff:]\n",
    "seven_testY = np.tile([0,0,0,0,0,0,0,1,0,0,0,0,0,0,0], (len(seven_testX),1))\n",
    "\n",
    "eight_cutoff = math.floor(len(eight_data) * train_percent)\n",
    "eight_trainX = eight_data[:eight_cutoff]\n",
    "eight_trainY = np.tile([0,0,0,0,0,0,0,0,1,0,0,0,0,0,0], (len(eight_trainX),1))\n",
    "eight_testX = eight_data[eight_cutoff:]\n",
    "eight_testY = np.tile([0,0,0,0,0,0,0,0,1,0,0,0,0,0,0], (len(eight_testX),1))\n",
    "\n",
    "nine_cutoff = math.floor(len(nine_data) * train_percent)\n",
    "nine_trainX = nine_data[:nine_cutoff]\n",
    "nine_trainY = np.tile([0,0,0,0,0,0,0,0,0,1,0,0,0,0,0], (len(nine_trainX),1))\n",
    "nine_testX = nine_data[nine_cutoff:]\n",
    "nine_testY = np.tile([0,0,0,0,0,0,0,0,0,1,0,0,0,0,0], (len(nine_testX),1))\n",
    "\n",
    "t_cutoff = math.floor(len(t_data) * train_percent)\n",
    "t_trainX = t_data[:t_cutoff]\n",
    "t_trainY = np.tile([0,0,0,0,0,0,0,0,0,0,1,0,0,0,0], (len(t_trainX),1))\n",
    "t_testX = t_data[t_cutoff:]\n",
    "t_testY = np.tile([0,0,0,0,0,0,0,0,0,0,1,0,0,0,0], (len(t_testX),1))\n",
    "\n",
    "h_cutoff = math.floor(len(h_data) * train_percent)\n",
    "h_trainX = h_data[:h_cutoff]\n",
    "h_trainY = np.tile([0,0,0,0,0,0,0,0,0,0,0,1,0,0,0], (len(h_trainX),1))\n",
    "h_testX = h_data[h_cutoff:]\n",
    "h_testY = np.tile([0,0,0,0,0,0,0,0,0,0,0,1,0,0,0], (len(h_testX),1))\n",
    "\n",
    "th_cutoff = math.floor(len(th_data) * train_percent)\n",
    "th_trainX = th_data[:th_cutoff]\n",
    "th_trainY = np.tile([0,0,0,0,0,0,0,0,0,0,0,0,1,0,0], (len(th_trainX),1))\n",
    "th_testX = th_data[th_cutoff:]\n",
    "th_testY = np.tile([0,0,0,0,0,0,0,0,0,0,0,0,1,0,0], (len(th_testX),1))\n",
    "\n",
    "tt_cutoff = math.floor(len(tt_data) * train_percent)\n",
    "tt_trainX = tt_data[:tt_cutoff]\n",
    "tt_trainY = np.tile([0,0,0,0,0,0,0,0,0,0,0,0,0,1,0], (len(tt_trainX),1))\n",
    "tt_testX = tt_data[tt_cutoff:]\n",
    "tt_testY = np.tile([0,0,0,0,0,0,0,0,0,0,0,0,0,1,0], (len(tt_testX),1))\n",
    "\n",
    "tm_cutoff = math.floor(len(tm_data) * train_percent)\n",
    "tm_trainX = tm_data[:tm_cutoff]\n",
    "tm_trainY = np.tile([0,0,0,0,0,0,0,0,0,0,0,0,0,0,1], (len(tm_trainX),1))\n",
    "tm_testX = tm_data[tm_cutoff:]\n",
    "tm_testY = np.tile([0,0,0,0,0,0,0,0,0,0,0,0,0,0,1], (len(tm_testX),1))\n",
    "\n",
    "\n",
    "# Concat arrays\n",
    "X_train = np.concatenate((zero_trainX, one_trainX, two_trainX, three_trainX, four_trainX, five_trainX, six_trainX, seven_trainX, eight_trainX, nine_trainX, t_trainX, h_trainX, th_trainX, tt_trainX, tm_trainX))\n",
    "Y_train = np.concatenate((zero_trainY, one_trainY, two_trainY, three_trainY, four_trainY, five_trainY, six_trainY, seven_trainY, eight_trainY, nine_trainY, t_trainY, h_trainY, th_trainY, tt_trainY, tm_trainY))\n",
    "X_test = np.concatenate((zero_testX, one_testX, two_testX, three_testX, four_testX, five_testX, six_testX, seven_testX, eight_testX, nine_testX, t_testX, h_testX, th_testX, tt_testX, tm_testX))\n",
    "Y_test = np.concatenate((zero_testY, one_testY, two_testY, three_testY, four_testY, five_testY, six_testY, seven_testY, eight_testY, nine_testY, t_testY, h_testY, th_testY, tt_testY, tm_testY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n",
      "4096\n"
     ]
    }
   ],
   "source": [
    "# normalize the X data\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "    \n",
    "print(len(X_train))\n",
    "print(len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (12000, 15) y_train shape: (12000, 15)\n",
      "12000 train set\n",
      "3000 test set\n"
     ]
    }
   ],
   "source": [
    "# Reshape input data from (64, 64) to (64, 64, 1)\n",
    "# Source: https://blog.tensorflow.org/2018/04/fashion-mnist-with-tfkeras.html#:~:text=(CNN)%20architecture.-,In%20just%20a%20few%20lines%20of%20code%2C%20you%20can%20define,10%20categories%20of%20handwritten%20digits).\n",
    "w, h = 64,64\n",
    "X_train = X_train.reshape(X_train.shape[0], w, h, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], w, h, 1)\n",
    "\n",
    "# Print training set shape\n",
    "print(\"x_train shape:\", Y_train.shape, \"y_train shape:\", Y_train.shape)\n",
    "\n",
    "# Print the number of training, validation, and test datasets\n",
    "print(X_train.shape[0], 'train set')\n",
    "print(X_test.shape[0], 'test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 64, 64, 64)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_183 (Dropout)        (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 32, 32, 32)        8224      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_184 (Dropout)        (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_224 (Dense)            (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_185 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_225 (Dense)            (None, 15)                3855      \n",
      "=================================================================\n",
      "Total params: 2,109,807\n",
      "Trainable params: 2,109,807\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Convolution neural network\n",
    "# Based on: https://colab.research.google.com/github/margaretmz/deep-learning/blob/master/fashion_mnist_keras.ipynb\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(64,64,1))) \n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(15, activation='softmax'))\n",
    "\n",
    "# Take a look at the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "375/375 [==============================] - ETA: 0s - loss: 1.2581 - accuracy: 0.6004\n",
      "Epoch 00001: val_loss improved from inf to 0.58732, saving model to model.weights.best.hdf5\n",
      "375/375 [==============================] - 65s 173ms/step - loss: 1.2581 - accuracy: 0.6004 - val_loss: 0.5873 - val_accuracy: 0.8240\n",
      "Epoch 2/15\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.6329 - accuracy: 0.7847\n",
      "Epoch 00002: val_loss improved from 0.58732 to 0.38499, saving model to model.weights.best.hdf5\n",
      "375/375 [==============================] - 64s 172ms/step - loss: 0.6329 - accuracy: 0.7847 - val_loss: 0.3850 - val_accuracy: 0.8793\n",
      "Epoch 3/15\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.4752 - accuracy: 0.8380\n",
      "Epoch 00003: val_loss improved from 0.38499 to 0.29695, saving model to model.weights.best.hdf5\n",
      "375/375 [==============================] - 65s 174ms/step - loss: 0.4752 - accuracy: 0.8380 - val_loss: 0.2969 - val_accuracy: 0.9077\n",
      "Epoch 4/15\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.3776 - accuracy: 0.8683\n",
      "Epoch 00004: val_loss improved from 0.29695 to 0.24403, saving model to model.weights.best.hdf5\n",
      "375/375 [==============================] - 62s 164ms/step - loss: 0.3776 - accuracy: 0.8683 - val_loss: 0.2440 - val_accuracy: 0.9220\n",
      "Epoch 5/15\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.3206 - accuracy: 0.8829\n",
      "Epoch 00005: val_loss improved from 0.24403 to 0.20193, saving model to model.weights.best.hdf5\n",
      "375/375 [==============================] - 59s 158ms/step - loss: 0.3206 - accuracy: 0.8829 - val_loss: 0.2019 - val_accuracy: 0.9390\n",
      "Epoch 6/15\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.9017\n",
      "Epoch 00006: val_loss did not improve from 0.20193\n",
      "375/375 [==============================] - 60s 161ms/step - loss: 0.2903 - accuracy: 0.9017 - val_loss: 0.2033 - val_accuracy: 0.9330\n",
      "Epoch 7/15\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2654 - accuracy: 0.9086\n",
      "Epoch 00007: val_loss improved from 0.20193 to 0.17361, saving model to model.weights.best.hdf5\n",
      "375/375 [==============================] - 58s 155ms/step - loss: 0.2654 - accuracy: 0.9086 - val_loss: 0.1736 - val_accuracy: 0.9410\n",
      "Epoch 8/15\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2208 - accuracy: 0.9237\n",
      "Epoch 00008: val_loss improved from 0.17361 to 0.15261, saving model to model.weights.best.hdf5\n",
      "375/375 [==============================] - 58s 154ms/step - loss: 0.2208 - accuracy: 0.9237 - val_loss: 0.1526 - val_accuracy: 0.9530\n",
      "Epoch 9/15\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2046 - accuracy: 0.9284\n",
      "Epoch 00009: val_loss improved from 0.15261 to 0.13922, saving model to model.weights.best.hdf5\n",
      "375/375 [==============================] - 57s 152ms/step - loss: 0.2046 - accuracy: 0.9284 - val_loss: 0.1392 - val_accuracy: 0.9530\n",
      "Epoch 10/15\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1898 - accuracy: 0.9334\n",
      "Epoch 00010: val_loss improved from 0.13922 to 0.13389, saving model to model.weights.best.hdf5\n",
      "375/375 [==============================] - 57s 152ms/step - loss: 0.1898 - accuracy: 0.9334 - val_loss: 0.1339 - val_accuracy: 0.9557\n",
      "Epoch 11/15\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1851 - accuracy: 0.9347\n",
      "Epoch 00011: val_loss improved from 0.13389 to 0.12306, saving model to model.weights.best.hdf5\n",
      "375/375 [==============================] - 57s 152ms/step - loss: 0.1851 - accuracy: 0.9347 - val_loss: 0.1231 - val_accuracy: 0.9590\n",
      "Epoch 12/15\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1567 - accuracy: 0.9458\n",
      "Epoch 00012: val_loss did not improve from 0.12306\n",
      "375/375 [==============================] - 58s 154ms/step - loss: 0.1567 - accuracy: 0.9458 - val_loss: 0.1313 - val_accuracy: 0.9563\n",
      "Epoch 13/15\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1580 - accuracy: 0.9466\n",
      "Epoch 00013: val_loss improved from 0.12306 to 0.11928, saving model to model.weights.best.hdf5\n",
      "375/375 [==============================] - 57s 152ms/step - loss: 0.1580 - accuracy: 0.9466 - val_loss: 0.1193 - val_accuracy: 0.9607\n",
      "Epoch 14/15\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1526 - accuracy: 0.9479\n",
      "Epoch 00014: val_loss improved from 0.11928 to 0.11009, saving model to model.weights.best.hdf5\n",
      "375/375 [==============================] - 57s 153ms/step - loss: 0.1526 - accuracy: 0.9479 - val_loss: 0.1101 - val_accuracy: 0.9643\n",
      "Epoch 15/15\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9540\n",
      "Epoch 00015: val_loss did not improve from 0.11009\n",
      "375/375 [==============================] - 63s 168ms/step - loss: 0.1375 - accuracy: 0.9540 - val_loss: 0.1278 - val_accuracy: 0.9613\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose = 1, save_best_only=True)\n",
    "\n",
    "# training the model\n",
    "history = model.fit(X_train, Y_train,\n",
    "          batch_size=32, epochs=15, \n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 0.9613333344459534\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test set\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
